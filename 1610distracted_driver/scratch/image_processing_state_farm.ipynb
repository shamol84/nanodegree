{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "f = open('/home/mohammad/img_label.p','rb')\n",
    "label=pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('/home/mohammad/img_data.p','rb')\n",
    "data=pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22424,), (22424, 50, 50))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12019, 50, 50) (3005, 50, 50) (7400, 50, 50) (7400,) (12019,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train1, X_test, y_train1, y_test = train_test_split(data, label, test_size=0.33, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train1, y_train1, test_size=0.2, random_state=42)\n",
    "print X_train.shape,X_valid.shape, X_test.shape,y_test.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_cross_validation(nfolds=10):\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 24, 32\n",
    "    batch_size = 64\n",
    "    nb_classes = 10\n",
    "    nb_epoch = 1\n",
    "    # number of convolutional filters to use\n",
    "    nb_filters = 32\n",
    "    # size of pooling area for max pooling\n",
    "    nb_pool = 2\n",
    "    # convolution kernel size\n",
    "    nb_conv = 3\n",
    "    random_state = 51\n",
    "\n",
    "    train_data, train_target = read_and_normalize_train_data(img_rows, img_cols)\n",
    "    test_data, test_id = read_and_normalize_test_data(img_rows, img_cols)\n",
    "\n",
    "    yfull_train = dict()\n",
    "    yfull_test = []\n",
    "    kf = KFold(len(train_data), n_folds=nfolds, shuffle=True, random_state=random_state)\n",
    "    num_fold = 0\n",
    "    for train_index, test_index in kf:\n",
    "        num_fold += 1\n",
    "        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n",
    "        X_train, X_valid = train_data[train_index], train_data[test_index]\n",
    "        Y_train, Y_valid = train_target[train_index], train_target[test_index]\n",
    "        print('Split train: ', len(X_train))\n",
    "        print('Split valid: ', len(X_valid))\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                                border_mode='valid',\n",
    "                                input_shape=(1, img_rows, img_cols)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(nb_classes))\n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "\n",
    "        model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                  show_accuracy=True, verbose=1, validation_data=(X_valid, Y_valid))\n",
    "\n",
    "        # score = model.evaluate(X_valid, Y_valid, show_accuracy=True, verbose=0)\n",
    "        # print('Score log_loss: ', score[0])\n",
    "\n",
    "        predictions_valid = model.predict(X_valid, batch_size=128, verbose=1)\n",
    "        score = log_loss(Y_valid, predictions_valid)\n",
    "        print('Score log_loss: ', score)\n",
    "\n",
    "        # Store valid predictions\n",
    "        for i in range(len(test_index)):\n",
    "            yfull_train[test_index[i]] = predictions_valid[i]\n",
    "\n",
    "        # Store test predictions\n",
    "        test_prediction = model.predict(test_data, batch_size=128, verbose=1)\n",
    "        yfull_test.append(test_prediction)\n",
    "\n",
    "    score = log_loss(train_target, dict_to_list(yfull_train))\n",
    "    print('Final score log_loss: ', score)\n",
    "\n",
    "    test_res = merge_several_folds_fast(yfull_test, nfolds)\n",
    "    create_submission(test_res, test_id, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('/home/mohammad/anaconda/lib/python2.7/site-packages')\n",
    "import keras\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (12019, 2500), (12019, 10))\n",
      "('Validation set', (3005, 2500), (3005, 10))\n"
     ]
    }
   ],
   "source": [
    "image_size=50\n",
    "num_labels=10\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  #labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  #labels = labels.reshape((-1,10))\n",
    "  return dataset, labels\n",
    "X_train,y_train = reformat(X_train, y_train)\n",
    "X_valid, y_valid = reformat(X_valid, y_valid)\n",
    "X_test, y_test = reformat(X_test, y_test)\n",
    "y_train = np_utils.to_categorical(y_train, num_labels)\n",
    "y_test = np_utils.to_categorical(y_test, num_labels)\n",
    "y_valid = np_utils.to_categorical(y_valid, num_labels)\n",
    "print('Training set', X_train.shape, y_train.shape)\n",
    "print('Validation set', X_valid.shape, y_valid.shape)\n",
    "#print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "#import numpy\n",
    "# fix random seed for reproducibility\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim=64, input_dim=2500))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12019 samples, validate on 3005 samples\n",
      "Epoch 1/10\n",
      "12019/12019 [==============================] - 1s - loss: 0.3102 - acc: 0.9388 - val_loss: 0.3982 - val_acc: 0.9002\n",
      "Epoch 2/10\n",
      "12019/12019 [==============================] - 1s - loss: 0.2740 - acc: 0.9468 - val_loss: 0.2921 - val_acc: 0.9371\n",
      "Epoch 3/10\n",
      "12019/12019 [==============================] - 1s - loss: 0.2460 - acc: 0.9537 - val_loss: 0.2565 - val_acc: 0.9517\n",
      "Epoch 4/10\n",
      "12019/12019 [==============================] - 1s - loss: 0.2199 - acc: 0.9591 - val_loss: 0.2478 - val_acc: 0.9484\n",
      "Epoch 5/10\n",
      "12019/12019 [==============================] - 1s - loss: 0.1996 - acc: 0.9634 - val_loss: 0.2264 - val_acc: 0.9488\n",
      "Epoch 6/10\n",
      "12019/12019 [==============================] - 1s - loss: 0.1840 - acc: 0.9668 - val_loss: 0.2322 - val_acc: 0.9494\n",
      "Epoch 7/10\n",
      "12019/12019 [==============================] - 1s - loss: 0.1659 - acc: 0.9712 - val_loss: 0.1951 - val_acc: 0.9624\n",
      "Epoch 8/10\n",
      "12019/12019 [==============================] - 1s - loss: 0.1540 - acc: 0.9722 - val_loss: 0.2350 - val_acc: 0.9428\n",
      "Epoch 9/10\n",
      "12019/12019 [==============================] - 1s - loss: 0.1427 - acc: 0.9762 - val_loss: 0.1770 - val_acc: 0.9637\n",
      "Epoch 10/10\n",
      "12019/12019 [==============================] - 1s - loss: 0.1328 - acc: 0.9770 - val_loss: 0.1944 - val_acc: 0.9564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4498605850>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "csv_logger = keras.callbacks.CSVLogger('/home/mohammad/training.log')\n",
    "model.fit(X_train, y_train, nb_epoch=10, batch_size=32,verbose=1,\n",
    "          show_accuracy=True, callbacks=[csv_logger],\n",
    "              validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2944/3005 [============================>.] - ETA: 0s('Score log_loss: ', 0.40482608707961254)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "predictions_valid = model.predict(X_valid, batch_size=128, verbose=1)\n",
    "score = log_loss(y_valid, predictions_valid)\n",
    "print('Score log_loss: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7040/7400 [===========================>..] - ETA: 0s('Score log_loss: ', 0.41561942186004425)\n"
     ]
    }
   ],
   "source": [
    "classes = model.predict_classes(X_test, batch_size=32)\n",
    "proba = model.predict_proba(X_test, batch_size=32)\n",
    "predictions_test = model.predict(X_test, batch_size=128, verbose=1)\n",
    "score = log_loss(y_test, predictions_test)\n",
    "print('Score log_loss: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test score:', 0.19712647291454108)\n",
      "('Test accuracy:', 0.95297297297297301)\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0144391550362\n"
     ]
    }
   ],
   "source": [
    "print log_loss(y_test[0],predictions_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.89999998,  0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(predictions_test[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loss_history = keras.callbacks.History\n",
    "a = np.array(['score','accuracy'])\n",
    "b = np.column_stack((a,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('/home/mohammad/test.txt', b, delimiter=\",\", fmt=\"%s\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
